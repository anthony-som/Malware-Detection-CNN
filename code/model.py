# %%
pip list

# %%
import torch
import torchvision
import torch.nn.functional as F
from torchvision import datasets, transforms, models
import torch.optim as optim
import torch.nn as nn
from torch.optim import lr_scheduler
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import pathlib
import copy
from torch.utils.tensorboard import SummaryWriter

# %%
torch.cuda.is_available() # check if gpu is availalbe

# %%
device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # set device to gpu if available else cpu
device

# %%
# Define the transformations for the input data
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# %%
# Load the dataset
train_data = torchvision.datasets.ImageFolder(root='./train', transform=transform)


# %%
train_iter = iter(train_data)
image, label = next(train_iter)
image.shape

# %%
## Splitting the dataset ##
# Define the proportions
train_ratio = 0.7
validation_ratio = 0.2
test_ratio = 0.1

# Calculate the sizes for each dataset
total_size = len(train_data)
train_size = int(train_ratio * total_size)
validation_size = int(validation_ratio * total_size)
test_size = total_size - train_size - validation_size

# Split the dataset
train_dataset, temp_dataset = torch.utils.data.random_split(train_data, [train_size, total_size - train_size])
validation_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [validation_size, test_size])

batch_size = 4

# Create data loaders for each set
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# %%
print(total_size), print(train_size), print(validation_size), print(test_size)

# %%
type(train_dataset), type(validation_dataset), type(test_dataset)

# %%
class NeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Simplified convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)
        
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)
        
        # Dynamically calculate the flattened size
        self._to_linear = None
        self._get_conv_output_size([3, 256, 256])  # Example input size
        
        # Fully connected layers
        self.fc1 = nn.Linear(self._to_linear, 512)
        self.drop1 = nn.Dropout(p=0.3)
        
        self.fc2 = nn.Linear(512, 256)
        self.drop2 = nn.Dropout(p=0.3)
        
        self.out = nn.Linear(256, 10)
        
    # Helper method to calculate the size of the flattened layer dynamically
    def _get_conv_output_size(self, shape):
        input = torch.rand(1, *shape)
        output_feat = self._forward_features(input)
        self._to_linear = np.prod(output_feat.size()[1:])
    
    # Forward pass through the convolutional layers only (used to calculate _to_linear)
    def _forward_features(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        return x
    
    # Full forward pass
    def forward(self, x):
        x = self._forward_features(x)
        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers
        x = F.relu(self.fc1(x))
        x = self.drop1(x)
        x = F.relu(self.fc2(x))
        x = self.drop2(x)
        x = self.out(x)
        return x


# %%
# Instantiate the network

net = NeuralNet()
net.to(device)

# %%
import torch.optim as optim

# Define a Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)  # You can adjust the learning rate as needed


# %%
def train_one_epoch():
    net.train()  # set the network to training mode
    
    running_loss = 0.0
    running_accuracy = 0.0
    
    for batch_index, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()  # Reset the gradients to zero
        
        outputs = net(inputs)
        _, predicted = torch.max(outputs, 1)
        correct = (predicted == labels).sum().item()
        running_accuracy += correct / inputs.size(0)
        
        loss = criterion(outputs, labels)
        running_loss += loss.item()
        loss.backward()
        optimizer.step()
        
        if batch_index % 500 == 499:  # print every 500 batches
            avg_loss_across_batches = running_loss / 500
            avg_acc_across_batches = (running_accuracy / 500) * 100
            print(f'Batch {batch_index + 1}, Loss: {avg_loss_across_batches:.3f}, Accuracy: {avg_acc_across_batches:.1f}%')
            running_loss = 0.0
            running_accuracy = 0.0

print()

# %%
def train_one_epoch():
    net.train()  # Set the network to training mode
    
    running_loss = 0.0
    total_correct = 0
    total_samples = 0
    
    for batch_index, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()  # Reset the gradients to zero
        
        outputs = net(inputs)
        _, predicted = torch.max(outputs, 1)
        total_correct += (predicted == labels).sum().item()
        total_samples += labels.size(0)
        
        loss = criterion(outputs, labels)
        running_loss += loss.item()
        loss.backward()
        optimizer.step()
        
        if batch_index % 50 == 49:  # Print every 50 batches
            avg_loss = running_loss / 50
            avg_accuracy = (total_correct / total_samples) * 100
            print(f'Batch {batch_index + 1}, Loss: {avg_loss:.3f}, Accuracy: {avg_accuracy:.1f}%')
            running_loss = 0.0
            total_correct = 0
            total_samples = 0


# %%
def validate_one_epoch():
    net.eval()  # Set the network to evaluation mode
    running_loss = 0.0
    total_correct = 0
    total_samples = 0
    
    with torch.no_grad():  # No gradients needed
        for inputs, labels in validation_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = net(inputs)
            _, predicted = torch.max(outputs, 1)
            total_correct += (predicted == labels).sum().item()
            total_samples += labels.size(0)
            
            loss = criterion(outputs, labels)
            running_loss += loss.item()
    
    avg_loss = running_loss / len(validation_loader)
    avg_accuracy = (total_correct / total_samples) * 100
    
    print(f'Validation Loss: {avg_loss:.3f}, Validation Accuracy: {avg_accuracy:.1f}%')
    print('***************************************************')


# %%
import math

num_batches = math.ceil(train_size / batch_size)
print(f"Total number of batches: {num_batches}")


# %%
num_epochs = 10

for epoch_index in range(num_epochs):
    print(f'Epoch: {epoch_index + 1}\n')
    
    train_one_epoch()
    validate_one_epoch()
    
print('Finished Training') 

# %%
print('Starting Testing')
test()

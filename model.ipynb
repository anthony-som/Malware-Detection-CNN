{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b71181a4-86b8-4799-89c0-c8c15b883030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- ------------\n",
      "absl-py                 2.1.0\n",
      "asttokens               2.4.1\n",
      "certifi                 2024.2.2\n",
      "charset-normalizer      3.3.2\n",
      "colorama                0.4.6\n",
      "comm                    0.2.1\n",
      "contourpy               1.2.0\n",
      "cycler                  0.12.1\n",
      "debugpy                 1.8.1\n",
      "decorator               5.1.1\n",
      "exceptiongroup          1.2.0\n",
      "executing               2.0.1\n",
      "fonttools               4.49.0\n",
      "grpcio                  1.62.0\n",
      "idna                    3.6\n",
      "importlib-metadata      7.0.1\n",
      "importlib_resources     6.1.2\n",
      "ipykernel               6.29.3\n",
      "ipython                 8.18.1\n",
      "jedi                    0.19.1\n",
      "joblib                  1.3.2\n",
      "jupyter_client          8.6.0\n",
      "jupyter_core            5.7.1\n",
      "kiwisolver              1.4.5\n",
      "Markdown                3.5.2\n",
      "MarkupSafe              2.1.5\n",
      "matplotlib              3.8.3\n",
      "matplotlib-inline       0.1.6\n",
      "nest-asyncio            1.6.0\n",
      "numpy                   1.26.4\n",
      "packaging               23.2\n",
      "pandas                  2.2.1\n",
      "parso                   0.8.3\n",
      "pillow                  10.2.0\n",
      "pip                     24.0\n",
      "platformdirs            4.2.0\n",
      "prompt-toolkit          3.0.43\n",
      "protobuf                4.25.3\n",
      "psutil                  5.9.8\n",
      "pure-eval               0.2.2\n",
      "Pygments                2.17.2\n",
      "pyparsing               3.1.1\n",
      "python-dateutil         2.8.2\n",
      "pytz                    2024.1\n",
      "pywin32                 306\n",
      "pyzmq                   25.1.2\n",
      "requests                2.31.0\n",
      "scikit-learn            1.4.1.post1\n",
      "scipy                   1.12.0\n",
      "setuptools              49.2.1\n",
      "six                     1.16.0\n",
      "stack-data              0.6.3\n",
      "tensorboard             2.16.2\n",
      "tensorboard-data-server 0.7.2\n",
      "threadpoolctl           3.3.0\n",
      "torch                   1.12.1+cu113\n",
      "torchaudio              0.12.1+cu113\n",
      "torchvision             0.13.1+cu113\n",
      "tornado                 6.4\n",
      "traitlets               5.14.1\n",
      "typing_extensions       4.10.0\n",
      "tzdata                  2024.1\n",
      "urllib3                 2.2.1\n",
      "wcwidth                 0.2.13\n",
      "Werkzeug                3.0.1\n",
      "zipp                    3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "993bd7ed-97e5-4db7-9b1a-ad54a3c72c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe205b38-c4ad-4308-8a62-13d4a406a65f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # check if gpu is availalbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1284dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # set device to gpu if available else cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863c3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations for the input data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "784c18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = torchvision.datasets.ImageFolder(root='./train', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab9f486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_data)\n",
    "image, label = next(train_iter)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68b3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset ##\n",
    "# Define the proportions\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the sizes for each dataset\n",
    "total_size = len(train_data)\n",
    "train_size = int(train_ratio * total_size)\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "test_size = total_size - train_size - validation_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, temp_dataset = torch.utils.data.random_split(train_data, [train_size, total_size - train_size])\n",
    "validation_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [validation_size, test_size])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create data loaders for each set\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44618c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6012\n",
      "4208\n",
      "1202\n",
      "602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_size), print(train_size), print(validation_size), print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09f93d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.Subset,\n",
       " torch.utils.data.dataset.Subset,\n",
       " torch.utils.data.dataset.Subset)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset), type(validation_dataset), type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f28266e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simplified convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dynamically calculate the flattened size\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output_size([3, 256, 256])  # Example input size\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    # Helper method to calculate the size of the flattened layer dynamically\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        output_feat = self._forward_features(input)\n",
    "        self._to_linear = np.prod(output_feat.size()[1:])\n",
    "    \n",
    "    # Forward pass through the convolutional layers only (used to calculate _to_linear)\n",
    "    def _forward_features(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "    \n",
    "    # Full forward pass\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec6fc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=262144, out_features=512, bias=True)\n",
       "  (drop1): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (drop2): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "\n",
    "net = NeuralNet()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c385c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # You can adjust the learning rate as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7bd3819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch():\n",
    "    net.train()  # set the network to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset the gradients to zero\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        running_accuracy += correct / inputs.size(0)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 500 == 499:  # print every 500 batches\n",
    "            avg_loss_across_batches = running_loss / 500\n",
    "            avg_acc_across_batches = (running_accuracy / 500) * 100\n",
    "            print(f'Batch {batch_index + 1}, Loss: {avg_loss_across_batches:.3f}, Accuracy: {avg_acc_across_batches:.1f}%')\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c299c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    net.train()  # Set the network to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset the gradients to zero\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 50 == 49:  # Print every 50 batches\n",
    "            avg_loss = running_loss / 50\n",
    "            avg_accuracy = (total_correct / total_samples) * 100\n",
    "            print(f'Batch {batch_index + 1}, Loss: {avg_loss:.3f}, Accuracy: {avg_accuracy:.1f}%')\n",
    "            running_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "511b5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    net.eval()  # Set the network to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(validation_loader)\n",
    "    avg_accuracy = (total_correct / total_samples) * 100\n",
    "    \n",
    "    print(f'Validation Loss: {avg_loss:.3f}, Validation Accuracy: {avg_accuracy:.1f}%')\n",
    "    print('***************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c13bae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 1052\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "num_batches = math.ceil(train_size / batch_size)\n",
    "print(f\"Total number of batches: {num_batches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ee9f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Batch 50, Loss: 2.527, Accuracy: 48.0%\n",
      "Batch 100, Loss: 0.841, Accuracy: 45.0%\n",
      "Batch 150, Loss: 0.699, Accuracy: 54.5%\n",
      "Batch 200, Loss: 0.624, Accuracy: 65.0%\n",
      "Batch 250, Loss: 0.644, Accuracy: 61.5%\n",
      "Batch 300, Loss: 0.472, Accuracy: 77.5%\n",
      "Batch 350, Loss: 0.288, Accuracy: 89.0%\n",
      "Batch 400, Loss: 0.228, Accuracy: 92.0%\n",
      "Batch 450, Loss: 0.406, Accuracy: 89.0%\n",
      "Batch 500, Loss: 0.231, Accuracy: 92.5%\n",
      "Batch 550, Loss: 0.264, Accuracy: 91.0%\n",
      "Batch 600, Loss: 0.272, Accuracy: 89.5%\n",
      "Batch 650, Loss: 0.181, Accuracy: 93.5%\n",
      "Batch 700, Loss: 0.181, Accuracy: 94.0%\n",
      "Batch 750, Loss: 0.132, Accuracy: 95.0%\n",
      "Batch 800, Loss: 0.181, Accuracy: 95.5%\n",
      "Batch 850, Loss: 0.216, Accuracy: 92.5%\n",
      "Batch 900, Loss: 0.160, Accuracy: 95.5%\n",
      "Batch 950, Loss: 0.095, Accuracy: 98.0%\n",
      "Batch 1000, Loss: 0.104, Accuracy: 97.0%\n",
      "Batch 1050, Loss: 0.108, Accuracy: 97.5%\n",
      "Validation Loss: 0.121, Validation Accuracy: 96.1%\n",
      "***************************************************\n",
      "Epoch: 2\n",
      "\n",
      "Batch 50, Loss: 0.102, Accuracy: 97.0%\n",
      "Batch 100, Loss: 0.127, Accuracy: 96.0%\n",
      "Batch 150, Loss: 0.060, Accuracy: 98.0%\n",
      "Batch 200, Loss: 0.080, Accuracy: 97.5%\n",
      "Batch 250, Loss: 0.067, Accuracy: 98.5%\n",
      "Batch 300, Loss: 0.103, Accuracy: 97.5%\n",
      "Batch 350, Loss: 0.126, Accuracy: 98.0%\n",
      "Batch 400, Loss: 0.091, Accuracy: 97.0%\n",
      "Batch 450, Loss: 0.056, Accuracy: 98.5%\n",
      "Batch 500, Loss: 0.080, Accuracy: 98.0%\n",
      "Batch 550, Loss: 0.093, Accuracy: 96.5%\n",
      "Batch 600, Loss: 0.034, Accuracy: 98.5%\n",
      "Batch 650, Loss: 0.049, Accuracy: 99.0%\n",
      "Batch 700, Loss: 0.087, Accuracy: 96.5%\n",
      "Batch 750, Loss: 0.105, Accuracy: 96.0%\n",
      "Batch 800, Loss: 0.143, Accuracy: 96.5%\n",
      "Batch 850, Loss: 0.063, Accuracy: 97.5%\n",
      "Batch 900, Loss: 0.106, Accuracy: 98.0%\n",
      "Batch 950, Loss: 0.068, Accuracy: 98.5%\n",
      "Batch 1000, Loss: 0.026, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.089, Accuracy: 97.0%\n",
      "Validation Loss: 0.096, Validation Accuracy: 96.4%\n",
      "***************************************************\n",
      "Epoch: 3\n",
      "\n",
      "Batch 50, Loss: 0.012, Accuracy: 100.0%\n",
      "Batch 100, Loss: 0.021, Accuracy: 99.0%\n",
      "Batch 150, Loss: 0.043, Accuracy: 98.0%\n",
      "Batch 200, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 250, Loss: 0.054, Accuracy: 99.0%\n",
      "Batch 300, Loss: 0.062, Accuracy: 98.0%\n",
      "Batch 350, Loss: 0.006, Accuracy: 100.0%\n",
      "Batch 400, Loss: 0.070, Accuracy: 98.5%\n",
      "Batch 450, Loss: 0.063, Accuracy: 98.5%\n",
      "Batch 500, Loss: 0.050, Accuracy: 98.5%\n",
      "Batch 550, Loss: 0.025, Accuracy: 99.0%\n",
      "Batch 600, Loss: 0.024, Accuracy: 99.0%\n",
      "Batch 650, Loss: 0.022, Accuracy: 99.0%\n",
      "Batch 700, Loss: 0.036, Accuracy: 98.5%\n",
      "Batch 750, Loss: 0.057, Accuracy: 98.5%\n",
      "Batch 800, Loss: 0.062, Accuracy: 99.0%\n",
      "Batch 850, Loss: 0.052, Accuracy: 99.5%\n",
      "Batch 900, Loss: 0.031, Accuracy: 98.5%\n",
      "Batch 950, Loss: 0.051, Accuracy: 98.0%\n",
      "Batch 1000, Loss: 0.065, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.020, Accuracy: 99.5%\n",
      "Validation Loss: 0.099, Validation Accuracy: 97.4%\n",
      "***************************************************\n",
      "Epoch: 4\n",
      "\n",
      "Batch 50, Loss: 0.023, Accuracy: 99.0%\n",
      "Batch 100, Loss: 0.027, Accuracy: 99.0%\n",
      "Batch 150, Loss: 0.012, Accuracy: 99.0%\n",
      "Batch 200, Loss: 0.021, Accuracy: 99.5%\n",
      "Batch 250, Loss: 0.009, Accuracy: 100.0%\n",
      "Batch 300, Loss: 0.009, Accuracy: 99.5%\n",
      "Batch 350, Loss: 0.019, Accuracy: 99.0%\n",
      "Batch 400, Loss: 0.015, Accuracy: 99.5%\n",
      "Batch 450, Loss: 0.006, Accuracy: 100.0%\n",
      "Batch 500, Loss: 0.013, Accuracy: 99.5%\n",
      "Batch 550, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 600, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 650, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 700, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 750, Loss: 0.026, Accuracy: 99.0%\n",
      "Batch 800, Loss: 0.020, Accuracy: 99.0%\n",
      "Batch 850, Loss: 0.064, Accuracy: 98.5%\n",
      "Batch 900, Loss: 0.029, Accuracy: 99.0%\n",
      "Batch 950, Loss: 0.082, Accuracy: 99.0%\n",
      "Batch 1000, Loss: 0.078, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.022, Accuracy: 99.5%\n",
      "Validation Loss: 0.117, Validation Accuracy: 97.6%\n",
      "***************************************************\n",
      "Epoch: 5\n",
      "\n",
      "Batch 50, Loss: 0.010, Accuracy: 99.5%\n",
      "Batch 100, Loss: 0.012, Accuracy: 100.0%\n",
      "Batch 150, Loss: 0.010, Accuracy: 99.5%\n",
      "Batch 200, Loss: 0.005, Accuracy: 99.5%\n",
      "Batch 250, Loss: 0.010, Accuracy: 99.5%\n",
      "Batch 300, Loss: 0.047, Accuracy: 98.0%\n",
      "Batch 350, Loss: 0.316, Accuracy: 96.0%\n",
      "Batch 400, Loss: 0.196, Accuracy: 93.0%\n",
      "Batch 450, Loss: 0.060, Accuracy: 98.5%\n",
      "Batch 500, Loss: 0.082, Accuracy: 97.5%\n",
      "Batch 550, Loss: 0.055, Accuracy: 99.0%\n",
      "Batch 600, Loss: 0.004, Accuracy: 100.0%\n",
      "Batch 650, Loss: 0.004, Accuracy: 100.0%\n",
      "Batch 700, Loss: 0.056, Accuracy: 98.5%\n",
      "Batch 750, Loss: 0.029, Accuracy: 99.0%\n",
      "Batch 800, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 850, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 900, Loss: 0.075, Accuracy: 98.0%\n",
      "Batch 950, Loss: 0.071, Accuracy: 99.5%\n",
      "Batch 1000, Loss: 0.072, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.071, Accuracy: 97.5%\n",
      "Validation Loss: 0.202, Validation Accuracy: 94.5%\n",
      "***************************************************\n",
      "Epoch: 6\n",
      "\n",
      "Batch 50, Loss: 0.035, Accuracy: 99.0%\n",
      "Batch 100, Loss: 0.063, Accuracy: 97.5%\n",
      "Batch 150, Loss: 0.059, Accuracy: 98.0%\n",
      "Batch 200, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 250, Loss: 0.045, Accuracy: 98.5%\n",
      "Batch 300, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 350, Loss: 0.047, Accuracy: 99.0%\n",
      "Batch 400, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 450, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 500, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 550, Loss: 0.027, Accuracy: 99.5%\n",
      "Batch 600, Loss: 0.013, Accuracy: 99.0%\n",
      "Batch 650, Loss: 0.032, Accuracy: 98.5%\n",
      "Batch 700, Loss: 0.055, Accuracy: 98.0%\n",
      "Batch 750, Loss: 0.012, Accuracy: 99.5%\n",
      "Batch 800, Loss: 0.027, Accuracy: 99.0%\n",
      "Batch 850, Loss: 0.037, Accuracy: 99.0%\n",
      "Batch 900, Loss: 0.024, Accuracy: 99.0%\n",
      "Batch 950, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 1000, Loss: 0.004, Accuracy: 99.5%\n",
      "Batch 1050, Loss: 0.000, Accuracy: 100.0%\n",
      "Validation Loss: 0.173, Validation Accuracy: 97.4%\n",
      "***************************************************\n",
      "Epoch: 7\n",
      "\n",
      "Batch 50, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 100, Loss: 0.114, Accuracy: 97.0%\n",
      "Batch 150, Loss: 0.037, Accuracy: 98.5%\n",
      "Batch 200, Loss: 0.007, Accuracy: 99.5%\n",
      "Batch 250, Loss: 0.005, Accuracy: 99.5%\n",
      "Batch 300, Loss: 0.091, Accuracy: 98.5%\n",
      "Batch 350, Loss: 0.015, Accuracy: 99.0%\n",
      "Batch 400, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 450, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 500, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 550, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 600, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 650, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 700, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 750, Loss: 0.003, Accuracy: 100.0%\n",
      "Batch 800, Loss: 0.025, Accuracy: 99.0%\n",
      "Batch 850, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 900, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 950, Loss: 0.005, Accuracy: 99.5%\n",
      "Batch 1000, Loss: 0.063, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.012, Accuracy: 99.5%\n",
      "Validation Loss: 0.156, Validation Accuracy: 96.8%\n",
      "***************************************************\n",
      "Epoch: 8\n",
      "\n",
      "Batch 50, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 100, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 150, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 200, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 250, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 300, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 350, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 400, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 450, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 500, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 550, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 600, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 650, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 700, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 750, Loss: 0.010, Accuracy: 99.5%\n",
      "Batch 800, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 850, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 900, Loss: 0.010, Accuracy: 99.5%\n",
      "Batch 950, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 1000, Loss: 0.032, Accuracy: 98.5%\n",
      "Batch 1050, Loss: 0.000, Accuracy: 100.0%\n",
      "Validation Loss: 0.652, Validation Accuracy: 96.8%\n",
      "***************************************************\n",
      "Epoch: 9\n",
      "\n",
      "Batch 50, Loss: 0.363, Accuracy: 91.5%\n",
      "Batch 100, Loss: 0.122, Accuracy: 96.5%\n",
      "Batch 150, Loss: 0.013, Accuracy: 99.0%\n",
      "Batch 200, Loss: 0.063, Accuracy: 99.0%\n",
      "Batch 250, Loss: 0.015, Accuracy: 99.5%\n",
      "Batch 300, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 350, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 400, Loss: 0.040, Accuracy: 98.5%\n",
      "Batch 450, Loss: 0.009, Accuracy: 99.5%\n",
      "Batch 500, Loss: 0.048, Accuracy: 99.0%\n",
      "Batch 550, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 600, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 650, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 700, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 750, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 800, Loss: 0.015, Accuracy: 99.5%\n",
      "Batch 850, Loss: 0.051, Accuracy: 98.5%\n",
      "Batch 900, Loss: 0.041, Accuracy: 99.0%\n",
      "Batch 950, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 1000, Loss: 0.021, Accuracy: 99.0%\n",
      "Batch 1050, Loss: 0.078, Accuracy: 98.5%\n",
      "Validation Loss: 0.339, Validation Accuracy: 96.8%\n",
      "***************************************************\n",
      "Epoch: 10\n",
      "\n",
      "Batch 50, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 100, Loss: 0.076, Accuracy: 98.5%\n",
      "Batch 150, Loss: 0.063, Accuracy: 96.5%\n",
      "Batch 200, Loss: 0.004, Accuracy: 99.5%\n",
      "Batch 250, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 300, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 350, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 400, Loss: 0.005, Accuracy: 100.0%\n",
      "Batch 450, Loss: 0.000, Accuracy: 100.0%\n",
      "Batch 500, Loss: 0.130, Accuracy: 98.5%\n",
      "Batch 550, Loss: 0.081, Accuracy: 96.5%\n",
      "Batch 600, Loss: 0.110, Accuracy: 97.5%\n",
      "Batch 650, Loss: 0.015, Accuracy: 99.0%\n",
      "Batch 700, Loss: 0.033, Accuracy: 99.0%\n",
      "Batch 750, Loss: 0.002, Accuracy: 100.0%\n",
      "Batch 800, Loss: 0.081, Accuracy: 99.5%\n",
      "Batch 850, Loss: 0.110, Accuracy: 96.5%\n",
      "Batch 900, Loss: 0.042, Accuracy: 99.0%\n",
      "Batch 950, Loss: 0.001, Accuracy: 100.0%\n",
      "Batch 1000, Loss: 0.003, Accuracy: 100.0%\n",
      "Batch 1050, Loss: 0.000, Accuracy: 100.0%\n",
      "Validation Loss: 0.324, Validation Accuracy: 97.2%\n",
      "***************************************************\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch_index in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_index + 1}\\n')\n",
    "    \n",
    "    train_one_epoch()\n",
    "    validate_one_epoch()\n",
    "    \n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe0d04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    avg_accuracy = (total_correct / total_samples) * 100\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.3f}, Test Accuracy: {avg_accuracy:.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34fb0c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Testing\n",
      "Test Loss: 0.192, Test Accuracy: 98.2%\n"
     ]
    }
   ],
   "source": [
    "print('Starting Testing')\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9-NN",
   "language": "python",
   "name": "3.9-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
